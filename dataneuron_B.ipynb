{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGv8lBCvk6CjplTUvX6DeQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanaharora/sanaharora/blob/main/dataneuron_B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi\n",
        "!pip install uvicorn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCJ3HKE_ifYH",
        "outputId": "efc4f156-f015-45b7-bc73-f0da170b3d4e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastapi\n",
            "  Downloading fastapi-0.110.1-py3-none-any.whl (91 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/91.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━\u001b[0m \u001b[32m81.9/91.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.9/91.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.6.4)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.11.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.16.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.38.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (1.2.0)\n",
            "Installing collected packages: starlette, fastapi\n",
            "Successfully installed fastapi-0.110.1 starlette-0.37.2\n",
            "Collecting uvicorn\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (4.11.0)\n",
            "Installing collected packages: h11, uvicorn\n",
            "Successfully installed h11-0.14.0 uvicorn-0.29.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI\n",
        "from pydantic import BaseModel\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "import collections\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer  # For Lemmetization of words\n",
        "from nltk.corpus import stopwords  # Load list of stopwords\n",
        "from nltk import word_tokenize # Convert paragraph in tokens\n",
        "\n",
        "import pickle\n",
        "import sys\n",
        "\n",
        "from gensim.models import word2vec # For represent words in vectors\n",
        "import gensim\n",
        "\n",
        "file_path = '/content/drive/MyDrive/DataNeuron_Text_Similarity.csv'\n",
        "text_data = pd.read_csv(file_path)\n",
        "print(\"Shape of text_data : \", text_data.shape)\n",
        "text_data.head(3)\n",
        "\n",
        "text_data.isnull().sum() # Check if text data have any null values\n",
        "\n",
        "# Preprocessing\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase\n",
        "\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Combining all the above stundents\n",
        "\n",
        "preprocessed_text1 = []\n",
        "\n",
        "# tqdm is for printing the status bar\n",
        "\n",
        "for sentance in tqdm(text_data['text1'].values):\n",
        "    sent = decontracted(sentance)\n",
        "    sent = sent.replace('\\\\r', ' ')\n",
        "    sent = sent.replace('\\\\\"', ' ')\n",
        "    sent = sent.replace('\\\\n', ' ')\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "\n",
        "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
        "    preprocessed_text1.append(sent.lower().strip())\n",
        "\n",
        "# Merging preprocessed_text1 in text_data\n",
        "\n",
        "text_data['text1'] = preprocessed_text1\n",
        "text_data.head(3)\n",
        "\n",
        "# Combining all the above stundents\n",
        "from tqdm import tqdm\n",
        "preprocessed_text2 = []\n",
        "\n",
        "# tqdm is for printing the status bar\n",
        "for sentance in tqdm(text_data['text2'].values):\n",
        "    sent = decontracted(sentance)\n",
        "    sent = sent.replace('\\\\r', ' ')\n",
        "    sent = sent.replace('\\\\\"', ' ')\n",
        "    sent = sent.replace('\\\\n', ' ')\n",
        "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "\n",
        "    sent = ' '.join(e for e in sent.split() if e not in stopwords.words('english'))\n",
        "    preprocessed_text2.append(sent.lower().strip())\n",
        "\n",
        "# Merging preprocessed_text2 in text_data\n",
        "\n",
        "text_data['text2'] = preprocessed_text2\n",
        "\n",
        "text_data.head(3)\n",
        "\n",
        "def word_tokenizer(text):\n",
        "            #tokenizes and stems the text\n",
        "            tokens = word_tokenize(text)\n",
        "            lemmatizer = WordNetLemmatizer()\n",
        "            tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "            return tokens\n",
        "\n",
        "import gensim\n",
        "\n",
        "wordmodelfile='/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz'\n",
        "wordmodel= gensim.models.KeyedVectors.load_word2vec_format(wordmodelfile, binary=True)\n",
        "\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "!pip install --upgrade gensim\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "similarity = [] # List for store similarity score\n",
        "\n",
        "\n",
        "\n",
        "for ind in text_data.index:\n",
        "\n",
        "        s1 = text_data['text1'][ind]\n",
        "        s2 = text_data['text2'][ind]\n",
        "\n",
        "        if s1==s2:\n",
        "                 similarity.append(0.0) # 0 means highly similar\n",
        "\n",
        "        else:\n",
        "\n",
        "            s1words = word_tokenize(s1)\n",
        "            s2words = word_tokenize(s2)\n",
        "\n",
        "\n",
        "\n",
        "            vocab = wordmodel.key_to_index #the vocabulary considered in the word embeddings\n",
        "\n",
        "            if len(s1words and s2words)==0:\n",
        "                    similarity.append(1.0)\n",
        "\n",
        "            else:\n",
        "\n",
        "                for word in s1words.copy(): #remove sentence words not found in the vocab\n",
        "                    if (word not in vocab):\n",
        "\n",
        "\n",
        "                            s1words.remove(word)\n",
        "\n",
        "\n",
        "                for word in s2words.copy(): #idem\n",
        "\n",
        "                    if (word not in vocab):\n",
        "\n",
        "                            s2words.remove(word)\n",
        "\n",
        "\n",
        "                similarity.append((1-wordmodel.n_similarity(s1words, s2words)))\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Get Unique_ID and similarity\n",
        "\n",
        "final_score = pd.DataFrame({'Similarity_score':similarity})\n",
        "final_score.head(3)\n",
        "\n",
        "final_score.to_csv('final_score.csv',index=False)\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Define request body model\n",
        "class TextPair(BaseModel):\n",
        "    text1: str\n",
        "    text2: str\n",
        "\n",
        "# Define API endpoint\n",
        "@app.post(\"/text-similarity/\")\n",
        "async def calculate_similarity(text_pair: TextPair):\n",
        "    text1 = text_pair.text1\n",
        "    text2 = text_pair.text2\n",
        "    similarity_score = text_similarity(text1, text2)\n",
        "    return {\"similarity_score\": similarity_score}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CGuqgfBid-2",
        "outputId": "1c306f3f-cbc8-43f7-a0eb-273e48a51e56"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Shape of text_data :  (3000, 2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "100%|██████████| 3000/3000 [02:29<00:00, 20.11it/s]\n",
            "100%|██████████| 3000/3000 [02:27<00:00, 20.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uS2uvQDjJWXX",
        "outputId": "bdfc0be5-ff1f-447b-a81f-1a9ffa7801d5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.1.6-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.1.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi nest-asyncio pyngrok uvicorn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teFZHiDFLwvg",
        "outputId": "1cabfe9a-40dc-4e26-e636-44d5fed5dfe0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.110.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (7.1.6)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.29.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.6.4)\n",
            "Requirement already satisfied: starlette<0.38.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.37.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.11.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.16.3)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.38.0,>=0.37.2->fastapi) (3.7.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (3.6)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.38.0,>=0.37.2->fastapi) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pyngrok\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "# Get your authtoken from https://dashboard.ngrok.com/get-started/your-authtoken\n",
        "auth_token = \"2fC653vOzhh6ih4hSW4p7Ff29Ob_7uMcy8qKZ4UJhJB3RGeAC\"\n",
        "\n",
        "# Set the authtoken\n",
        "ngrok.set_auth_token(auth_token)\n",
        "\n",
        "# Connect to ngrok\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "\n",
        "# Print the public URL\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "\n",
        "# Apply nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "app= None\n",
        "# Run the uvicorn server\n",
        "import asyncio\n",
        "async def main():\n",
        "  await uvicorn.run(app,host=\"0.0.0.0\", port=8000)\n",
        "asyncio.run(main(), debug=True)"
      ],
      "metadata": {
        "id": "_fkNftjgMLyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Define the URL of the API endpoint\n",
        "url = \"https://bc3d-34-141-245-65.ngrok-free.app/\"\n",
        "\n",
        "# Define the text pairs for similarity calculation\n",
        "text_pair = {\n",
        "    \"text1\": \"First text to compare\",\n",
        "    \"text2\": \"Second text to compare\"\n",
        "}\n",
        "\n",
        "# Send a POST request to the API endpoint with the text pair\n",
        "response = requests.post(url, json=text_pair)\n",
        "\n",
        "# Extract the similarity score from the response\n",
        "similarity_score = response.json()[\"similarity_score\"]\n",
        "\n",
        "# Print the similarity score\n",
        "print(\"Similarity score:\", similarity_score)\n"
      ],
      "metadata": {
        "id": "1ei4IzuCOOJr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}